{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2a3596b-9432-4a77-bddb-8006b3c4a07c",
   "metadata": {},
   "source": [
    " ## Intro\n",
    " \n",
    "We all know LLM models are word generators. When you ask Chat GPT \"hello, can you help me write a blog post about xyz\", behind the curtain it generates next word (more specifically tokens) and then whole sentence is fed back to LLM and it generates another next word and process is repeated again and again, But have you ever wondered how exactly it works? \n",
    " \n",
    "In this blog post, We will dig deeper into building blocks of Large Language Models and how they come together. No hard Math or coding experience is needed, All you need is Attention and some familiarity with 10th level math i.e. what is matrix, multiplication, simple equation and basic python coding would be a plus but not necessary. \n",
    " \n",
    " ### Let's build a TLM (Tiny Language Model)\n",
    " \n",
    "Before we get started, I would like to set expectation right. Tiny LM is not going to be state of Art model that you can ask, how to cure cancer and it spits out the exact procedure. On second thought, even Chat GPT can't do that yet but you get the point. \n",
    "\n",
    "Tiny LM as name suggests is going to be a very small model that predicts next character based on the data we will train it. So it won't even generate real english words. We will get there in next blog post but before that we need to understand how to build model that generates text which seems like English but is gibberish. \n",
    "\n",
    "We will use a dataset called \"Tiny Open Domain Books\" and it contains text from four books \n",
    "\n",
    "- Alice in Wonderland - Lewis Caroll\n",
    "- Dracula - Bram Stoker\n",
    "- The Wonderful Wizard of Oz - L. Frank Baum\n",
    "- The Count of Monte Cristo - Alexandre Dumas & Auguste Maquet\n",
    "\n",
    "this file is around 3MB and can be downloaded from here. [TinyBook](https://huggingface.co/datasets/Blackroot/Tiny-Open-Domain-Books) \n",
    "\n",
    "\n",
    "#### Read Text File\n",
    "\n",
    "Let's load the text file and see what it contains. It has ~245K characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64e9c885-d17e-4eec-a22b-dc1dbd2aebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tinyBook = open('../../data/tinystories.txt', 'r').read()  # Read the tiny book data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92f57b3b-e77c-45cb-b371-3d54016cb502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244539"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tinyBook) # Total lenth of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd241a4-f275-407e-b83c-fd243239787e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Dorothy lived in the midst of the great Kansas prairies, with\\nUncle Henry, who was a farmer, and Aunt Em, who was the farmer's\\nwife. Their house was small, for the lumber to build it had to be\\ncarried by wagon many miles. There were four walls, a floor and a\\nroof, which made one room; and this room contained a rusty looking\\ncookstove, a cupboard for the dishes, a table, three or four\\nchairs, and the beds. Uncle Henry and Aunt Em had a big bed in one\\ncorner, and Dorothy a little bed in another corner. There was no\\ngarret at all, and no cellar—except a small hole dug in the ground,\\ncalled a cyclone cellar, where the family could go in case one of\\nthose great whirlwinds arose, mighty enough to crush any building\\nin its path. It was reached by a trap door in the middle of the\\nfloor, from which a ladder led down into the small, dark hole.\\nWhen Dorothy stood in the doorway and looked around, she could\\nsee nothing but the great gray prairie on every side. Not a tree\\nnor a house broke the broa\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tinyBook[:1000] # First 1000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c76bd88-c84f-4a65-b6af-8b225809a36e",
   "metadata": {},
   "source": [
    "Now let's see how many unique characters are in this text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34d2b716-37de-4c4d-b782-59fcc643b5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n",
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'Y', 'Z', '[', ']', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\xa0', '—', '‘', '’', '“', '”', '…']\n"
     ]
    }
   ],
   "source": [
    "uniqueCharacters = sorted(list(set(''.join(tinyBook))))\n",
    "print(len(uniqueCharacters))\n",
    "print(uniqueCharacters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66805bac-3cea-4012-a5b6-d815c91737f3",
   "metadata": {},
   "source": [
    "#### Encode - Decode\n",
    "\n",
    "We have 86 unique characters. However we need to convert these to numbers before they can be fed to any model. That's how it works with any LLM model. LLM never sees text you type in directly. It gets converted to numbers and model splits out numbers and then those numbers are converted to text. No magic or AI counsiousness. \n",
    "\n",
    "This process of converting text to numbers is called tokenization. Easiest tokenization method would be to just use index of character. So '\\n' will be 0 and ' ' will 1 and '!' will be 2 and so on. \n",
    "\n",
    "Following code creates two mappings, `character_to_index` which is basically {\"character\": \"number\"} and `index_to_character` which is reverse {\"index\": \"characters\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ffb8d7c-51d2-4dfe-b45a-83b4bdcb9e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: '\\n', 2: ' ', 3: '!', 4: '\"', 5: '&', 6: \"'\", 7: '(', 8: ')', 9: '*', 10: ',', 11: '-', 12: '.', 13: '0', 14: '1', 15: '2', 16: '3', 17: '4', 18: '5', 19: '6', 20: '7', 21: '8', 22: '9', 23: ':', 24: ';', 25: '?', 26: 'A', 27: 'B', 28: 'C', 29: 'D', 30: 'E', 31: 'F', 32: 'G', 33: 'H', 34: 'I', 35: 'J', 36: 'K', 37: 'L', 38: 'M', 39: 'N', 40: 'O', 41: 'P', 42: 'Q', 43: 'R', 44: 'S', 45: 'T', 46: 'U', 47: 'V', 48: 'W', 49: 'Y', 50: 'Z', 51: '[', 52: ']', 53: '`', 54: 'a', 55: 'b', 56: 'c', 57: 'd', 58: 'e', 59: 'f', 60: 'g', 61: 'h', 62: 'i', 63: 'j', 64: 'k', 65: 'l', 66: 'm', 67: 'n', 68: 'o', 69: 'p', 70: 'q', 71: 'r', 72: 's', 73: 't', 74: 'u', 75: 'v', 76: 'w', 77: 'x', 78: 'y', 79: 'z', 80: '\\xa0', 81: '—', 82: '‘', 83: '’', 84: '“', 85: '”', 86: '…'}\n"
     ]
    }
   ],
   "source": [
    "character_to_index = {s:i+1 for i,s in enumerate(uniqueCharacters)}\n",
    "index_to_character = {i:s for s,i in character_to_index.items()}\n",
    "print(index_to_character)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40355c4a-e8b3-4932-bcf9-4065e6cfb8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dorothy --> [29, 68, 71, 68, 73, 61, 78]\n",
      "[29, 68, 71, 68, 73, 61, 78] --> Dorothy\n"
     ]
    }
   ],
   "source": [
    "encode = lambda s: [character_to_index[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([index_to_character[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "print('Dorothy -->', encode(\"Dorothy\"))\n",
    "print('[29, 68, 71, 68, 73, 61, 78] -->', decode([29, 68, 71, 68, 73, 61, 78]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b5aed-2d57-40a9-871c-4cd0d4a6ec59",
   "metadata": {},
   "source": [
    "encode is a function that takes a text and returns a list with index of each character in text. Similarly decode function takes list of numbers and return text.\n",
    "\n",
    "\n",
    "#### Context Window\n",
    "Next we define how many character models sees before generating next one. This is known as block size or context window. For simplicity let's say block_size is 3. which would mean if we input `Dor` next letter we expect is `o` and for `Doro`, we expect `t` and so on. \n",
    "\n",
    "When `Doro` is input to model, model will only consider `oro` to generate `t` and that's how it works with LLMs too. If hypothetically you could insert text more text than context window on Chat GPT, it would only consider text of it's context window size and will completly ignore anything before. That's why Chat GPT does not allow that long text to be entered and give an error.\n",
    "\n",
    "`The message you submitted was too long, please submit something shorter.`\n",
    "\n",
    "On other hand, we are allowed to enter text that is less than context window. Input `D` should return `o` and `Do` should return `t` and so on.\n",
    "\n",
    "block_size = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63270b57-1581-4d36-a334-eb7a6aff8fff",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Building Training Data\n",
    "\n",
    "Let's think this through, what is traning data? We want to tell model that when input `Dor`, we expect `o`, in other words when input is `[29, 68, 71]`, output expected is `[68]` \n",
    "\n",
    "Even a simple world like `Dorothy` (`[29, 68, 71, 68, 73, 61, 78]`) would have multiple training set. \n",
    "\n",
    "- `[29]` -> `[68]`           ---------------- ('D' -> 'o')\n",
    "- `[29, 68]` -> `[71]`       ----------- ('Do' -> 'r')\n",
    "- `[29, 68, 71]` -> `[68]`   ------ ('Dor' -> 'o')\n",
    "- `[68, 71, 68]` -> `[73]`   ------ ('oro' -> 't')\n",
    "- `[71, 68, 73]` -> `[61]`   ------ ('rot' -> 'h')\n",
    "- `[68, 73, 61]` -> `[78]`   ------ ('oth' -> 'y')\n",
    "\n",
    "We don't feed all data during training, because training happens over multiple steps and each step is costly resource wise. Also turns out, whether we train on whole data during each step or if we do training step in batches, we get similar results. So we will pick training data in batches randomly for each step. Batch size will be 8. Which mean 8 set of examples like we see above.\n",
    "\n",
    "\n",
    "Note: we will not split in multiple sets like train and validation, for simplicity, however it is an important step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52bda7ca-16f9-46eb-a3a4-e81a1ab690a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b20a88-f406-4ac6-abc1-0305e4301f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64 torch.Size([244539])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "block_size = 3\n",
    "data = torch.tensor(encode(tinyBook), dtype=torch.long)\n",
    "print(data.dtype, data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad15aaa6-b385-4383-be4f-e72651d44e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function selects training batch randomly \n",
    "def get_batch():\n",
    "    ix = torch.randint(len(data)-block_size , (batch_size,))     # Generate random numbers  \n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c917c9-88a5-4cea-8c43-7e9f948394f8",
   "metadata": {},
   "source": [
    "Let's inspect a batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66bf1a6e-26c5-43fc-a8df-6c852cbcba61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[62, 73, 58],\n",
       "        [62, 67, 72],\n",
       "        [68, 71,  2],\n",
       "        [68,  2, 68],\n",
       "        [73, 61, 68],\n",
       "        [54,  2, 76],\n",
       "        [44, 68, 74],\n",
       "        [67, 73, 62]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_batch()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfa81ea6-1868-4340-8304-d406d8934fbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[73, 58,  2],\n",
       "        [67, 72, 73],\n",
       "        [71,  2, 54],\n",
       "        [ 2, 68, 67],\n",
       "        [61, 68, 74],\n",
       "        [ 2, 76, 58],\n",
       "        [68, 74, 69],\n",
       "        [73, 62, 68]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7e136-7229-46b0-a6cb-3680e2818255",
   "metadata": {},
   "source": [
    "We are packing lots of information here. If we only focus on first row, We are telling model, when input is `76`, expected output is `58`, when input is `76,58`, expected output is `67` and when input is `76, 58, 67`, expected output is `73`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a93af-bf51-4ecb-aa0f-f0968abc33af",
   "metadata": {},
   "source": [
    "#### Embedding\n",
    "\n",
    "Let's take it up a notch. You must have heard term `parameters` with LLM. For example, LLAMA3-70B has 70 Billion parameters. Parameters are like variable which are set to certain numbers during the training. There is common analogy to think parameters as dials in a machine that needs to be set to certain number in order to get desired output.\n",
    "\n",
    "Embedding is a set of one of these trainable parameters. Each character we have can be streched to a vector, to add more dimension. Wait what? What does it even mean?\n",
    "\n",
    "In other words we assign each character, a list of random numbers so for eg. 'A' character which has index `26` in our mapping, could be assigned  `[0.4, 0.3, 0.2, 0.1, 0.1]` Embedding can be think of as giving multiple features to a character, So here we can say `A` can have five features and later model will figure out what are good numbers of these features to make sense with data. Number of features or Embedding dimension is a hyper-parameter, similar to block_size or batch_size, these numbers we choose and fix manually.\n",
    "\n",
    "We are setting Embedding Dimension to 5 and our vocab size is 86. so we generate a matrix of 86 rows and 5 columns. Each row contains embeddings for that a character. So if we want to see embedding for character `A`, we will just check 26th row.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69a9d49d-e960-42f2-8045-9fcbd383f525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Embedding Matrix:  torch.Size([86, 5])\n"
     ]
    }
   ],
   "source": [
    "embedding_dimension = 5\n",
    "vocab_size = 86\n",
    "embedding_matrix = torch.randn((vocab_size, embedding_dimension))\n",
    "print('Shape of Embedding Matrix: ', embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa06eba2-f3fe-4f4b-988e-f56fe0a83054",
   "metadata": {},
   "source": [
    "For a batch `x` we would want to select it's embedding and that can be done with simply indexing embedding_matrix with `x`. Thanks to pytorch, it is a framework in python, which makes all these complex matrix operations a breeze. So our input embedding matrix is now three dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cc67ba1-f1cd-4665-bcd8-4372ddc23933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "embeddings = embedding_matrix[x]\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dd4acb-44cb-4d10-8f10-763a4d74a209",
   "metadata": {},
   "source": [
    "#### Neuron\n",
    "\n",
    "So far we have prepared the input, next step is adding a neural layer to mix. Neuron is mathematically described as `Wx + b`. `W` here represents weights, `x` is input and `b` is bias. \n",
    "\n",
    "We won't just use one neuron, we would like to use a large number of neurons, let's say 100. These hundred neuron will collectively be a layer. We can stack as many of these layer. For simplicity we will use two neural layers.\n",
    "\n",
    "[Insert Image of Neuron/NN ?]\n",
    "\n",
    "\n",
    "\n",
    "Weights and biases are another set of parameters that are trained or in other words, they start with random numbers and we perform a set calculations to modify (tune) these numbers to make final output of Neural Network closer to actual output.\n",
    "\n",
    "First layer would have 100 neurons, then this layer will pass through activation function and to second layer.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "570efc51-fb15-40b9-bbc4-30e082752424",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.rand(15, 100)\n",
    "b1 = torch.rand(100)\n",
    "first_layer_pre_activation = embeddings.view(-1, 15) @ W1 + b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "237f90e4-ac72-47b8-86e3-5947f27343a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_layer_output = torch.tanh(first_layer_pre_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3debe54b-ec6a-4ea1-a424-530bfdd6b2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.rand(100, vocab_size)\n",
    "b2 = torch.rand(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a57f6a0f-b98c-4987-aff0-81553467b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = first_layer_output @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d0143e86-dbf0-45d4-b097-d9a8046b6b24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 86])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3cf53ca6-a5fb-4c7a-a29b-d4a45b8cf5b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/fastai/lib/python3.10/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "loss = F.cross_entropy(logits,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7dbb9ca-dde8-4165-b624-48d6c7cc0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c229dd2-6f3d-4f04-ba8e-3483a53f82da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6.0203e-04, 1.9227e-02, 2.4898e-03, 3.4971e-02, 3.1267e-05, 5.3999e-04,\n",
       "        1.4671e-03, 1.0701e-03, 2.1355e-04, 2.1248e-02, 1.0419e-02, 7.6492e-06,\n",
       "        1.5678e-03, 1.2366e-03, 4.6987e-04, 2.3232e-04, 9.1721e-03, 1.6285e-02,\n",
       "        5.4651e-03, 8.8872e-05, 2.8982e-03, 3.1337e-04, 1.3703e-02, 1.7932e-03,\n",
       "        4.8126e-05, 7.3775e-05, 1.7926e-03, 1.3406e-04, 3.0437e-02, 1.7552e-03,\n",
       "        7.7340e-04, 1.4197e-05, 2.4941e-04, 7.9453e-03, 1.2898e-05, 6.0568e-05,\n",
       "        2.0386e-04, 5.9326e-05, 1.3861e-02, 5.2173e-04, 4.4592e-02, 6.7000e-03,\n",
       "        1.3556e-03, 9.7609e-04, 1.6482e-02, 1.0915e-04, 5.2550e-04, 5.5785e-03,\n",
       "        5.6149e-03, 8.5327e-05, 5.0739e-04, 2.0296e-06, 2.9259e-05, 2.5029e-05,\n",
       "        6.7468e-02, 2.5599e-02, 5.8128e-05, 5.2848e-03, 8.9373e-03, 6.3425e-02,\n",
       "        3.9879e-03, 2.8140e-02, 5.0439e-03, 2.0298e-04, 2.6321e-05, 2.2958e-06,\n",
       "        1.1130e-02, 4.2381e-03, 3.2411e-04, 6.6507e-02, 3.4426e-04, 1.2019e-05,\n",
       "        1.1563e-05, 1.2994e-03, 1.4782e-04, 5.3631e-05, 2.5233e-04, 1.0327e-04,\n",
       "        3.4255e-01, 2.7875e-05, 7.0727e-02, 5.9788e-04, 6.2636e-03, 5.7700e-04,\n",
       "        7.2965e-05, 5.4665e-04])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43709fe6-2ec1-4658-9893-09cab998504b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
